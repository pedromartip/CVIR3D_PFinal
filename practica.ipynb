{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_find_features(image_path, scale_percent=30):\n",
    "    \"\"\"\n",
    "    Read an image from the given path, resize it by a percentage, and detect keypoints and descriptors using ORB.\n",
    "\n",
    "    Args:\n",
    "    - image_path (str): The path to the image file.\n",
    "    - scale_percent (int): The percent (relative to the original image size) to which the image should be scaled.\n",
    "\n",
    "    Returns:\n",
    "    - keypoints (list of cv2.KeyPoint): The detected keypoints in the image.\n",
    "    - descriptors (np.ndarray): The feature descriptors corresponding to the keypoints.\n",
    "    - resized_image (np.ndarray): The resized image read from the file in BGR format.\n",
    "    \"\"\"\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Image at path {image_path} could not be read.\")\n",
    "\n",
    "    # Calculate the new dimensions\n",
    "    width = int(image.shape[1] * scale_percent / 100)\n",
    "    height = int(image.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, dim, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Find the keypoints and descriptors with ORB\n",
    "    keypoints, descriptors = orb.detectAndCompute(resized_image, None)\n",
    "    \n",
    "    return keypoints, descriptors, resized_image\n",
    "\n",
    "\n",
    "def match_features(descriptors1, descriptors2):\n",
    "    \"\"\"\n",
    "    Match feature descriptors between two sets of descriptors using the BFMatcher with NORM_HAMMING.\n",
    "\n",
    "    Args:\n",
    "    - descriptors1 (np.ndarray): Feature descriptors of the first image.\n",
    "    - descriptors2 (np.ndarray): Feature descriptors of the second image.\n",
    "\n",
    "    Returns:\n",
    "    - matches (list of cv2.DMatch): The best matches found between the two descriptor sets.\n",
    "    \n",
    "    The function initializes a BFMatcher object with the NORM_HAMMING metric, suitable for ORB features,\n",
    "    and uses it to find the best matches between two descriptor sets.\n",
    "    \"\"\"\n",
    "    # Create BFMatcher object\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    \n",
    "    # Match descriptors\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    \n",
    "    # Sort them in the order of their distance (the lower the better)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def stitch_images(image1, keypoints1, image2, keypoints2, matches):\n",
    "    \"\"\"\n",
    "    Stitch two images into a panorama using the matches of keypoints between them.\n",
    "\n",
    "    Args:\n",
    "    - image1 (np.ndarray): The first image to stitch.\n",
    "    - keypoints1 (list of cv2.KeyPoint): Keypoints of the first image.\n",
    "    - image2 (np.ndarray): The second image to stitch.\n",
    "    - keypoints2 (list of cv2.KeyPoint): Keypoints of the second image.\n",
    "    - matches (list of cv2.DMatch): Matches between the keypoints of the two images.\n",
    "\n",
    "    Returns:\n",
    "    - panorama (np.ndarray): The stitched panorama image.\n",
    "    \n",
    "    This function computes the homography matrix using the keypoints of the matches found. It then warps the\n",
    "    perspective of the second image to align with the first image and stitches them together. The resulting image is\n",
    "    the panorama of the two images.\n",
    "    \"\"\"\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    try:\n",
    "        h, mask = cv2.findHomography(points2, points1, cv2.RANSAC)\n",
    "    except cv2.error as e:\n",
    "        print(f\"Error computing homography: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Use homography\n",
    "    height, width, channels = image1.shape\n",
    "    panorama = cv2.warpPerspective(image2, h, (width * 2, height))\n",
    "    \n",
    "    # Stitch the images together\n",
    "    panorama[0:image1.shape[0], 0:image1.shape[1]] = image1\n",
    "\n",
    "    return panorama\n",
    "\n",
    "def create_panorama(image_left, image_mid, image_right, keypoints_left, keypoints_mid, keypoints_right, descriptors_left, descriptors_mid, descriptors_right):\n",
    "    \"\"\"\n",
    "    Create a panorama with the middle image as the reference, stitching the left and right images to it.\n",
    "\n",
    "    Args:\n",
    "    - image_left (np.ndarray): The left image to be stitched.\n",
    "    - image_mid (np.ndarray): The middle reference image.\n",
    "    - image_right (np.ndarray): The right image to be stitched.\n",
    "    - keypoints_left (list of cv2.KeyPoint): Keypoints of the left image.\n",
    "    - keypoints_mid (list of cv2.KeyPoint): Keypoints of the middle image.\n",
    "    - keypoints_right (list of cv2.KeyPoint): Keypoints of the right image.\n",
    "    - descriptors_left (np.ndarray): Descriptors of the left image.\n",
    "    - descriptors_mid (np.ndarray): Descriptors of the middle image.\n",
    "    - descriptors_right (np.ndarray): Descriptors of the right image.\n",
    "\n",
    "    Returns:\n",
    "    - panorama (np.ndarray): The final stitched panorama image.\n",
    "    \n",
    "    The function first matches features between the middle and left images, and then between the middle and right images.\n",
    "    It stitches the left image to the middle, then stitches the right image to the combined left and middle images, \n",
    "    all the while keeping the middle image as the central, unaltered reference.\n",
    "    \"\"\"\n",
    "    # Match features between middle and left images, then stitch them\n",
    "    matches_left_mid = match_features(descriptors_left, descriptors_mid)\n",
    "    left_to_mid = stitch_images(image_left, keypoints_left, image_mid, keypoints_mid, matches_left_mid)\n",
    "\n",
    "    # Match features between middle and right images, then stitch them\n",
    "    matches_right_mid = match_features(descriptors_right, descriptors_mid)\n",
    "    right_to_mid = stitch_images(image_mid, keypoints_mid, image_right, keypoints_right, matches_right_mid)\n",
    "\n",
    "    # The right_to_mid image will have extra width to accommodate the warped right image\n",
    "    # We need to crop this before combining with the left_to_mid image\n",
    "    right_to_mid_cropped = right_to_mid[:, 0:image_mid.shape[1]]\n",
    "\n",
    "    # Combine the left_to_mid and right_to_mid images to create the full panorama\n",
    "    # Note that the dimensions of left_to_mid and right_to_mid_cropped must match exactly for this to work\n",
    "    panorama = np.concatenate((left_to_mid, right_to_mid_cropped), axis=1)\n",
    "\n",
    "    return panorama\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m keypoints_right, descriptors_right, image_right \u001b[38;5;241m=\u001b[39m read_and_find_features(H_filenames[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create the panorama\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m final_panorama \u001b[38;5;241m=\u001b[39m create_panorama(image_left, image_mid, image_right, keypoints_left, keypoints_mid, keypoints_right, descriptors_left, descriptors_mid, descriptors_right)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Display the final panorama\u001b[39;00m\n\u001b[0;32m     16\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Panorama\u001b[39m\u001b[38;5;124m'\u001b[39m, final_panorama)\n",
      "Cell \u001b[1;32mIn[12], line 127\u001b[0m, in \u001b[0;36mcreate_panorama\u001b[1;34m(image_left, image_mid, image_right, keypoints_left, keypoints_mid, keypoints_right, descriptors_left, descriptors_mid, descriptors_right)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Match features between middle and right images, then stitch them\u001b[39;00m\n\u001b[0;32m    126\u001b[0m matches_right_mid \u001b[38;5;241m=\u001b[39m match_features(descriptors_right, descriptors_mid)\n\u001b[1;32m--> 127\u001b[0m right_to_mid \u001b[38;5;241m=\u001b[39m stitch_images(image_mid, keypoints_mid, image_right, keypoints_right, matches_right_mid)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# The right_to_mid image will have extra width to accommodate the warped right image\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# We need to crop this before combining with the left_to_mid image\u001b[39;00m\n\u001b[0;32m    131\u001b[0m right_to_mid_cropped \u001b[38;5;241m=\u001b[39m right_to_mid[:, \u001b[38;5;241m0\u001b[39m:image_mid\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[12], line 84\u001b[0m, in \u001b[0;36mstitch_images\u001b[1;34m(image1, keypoints1, image2, keypoints2, matches)\u001b[0m\n\u001b[0;32m     81\u001b[0m points2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(matches), \u001b[38;5;241m2\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(matches):\n\u001b[1;32m---> 84\u001b[0m     points1[i, :] \u001b[38;5;241m=\u001b[39m keypoints1[match\u001b[38;5;241m.\u001b[39mqueryIdx]\u001b[38;5;241m.\u001b[39mpt\n\u001b[0;32m     85\u001b[0m     points2[i, :] \u001b[38;5;241m=\u001b[39m keypoints2[match\u001b[38;5;241m.\u001b[39mtrainIdx]\u001b[38;5;241m.\u001b[39mpt\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Find homography\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Nombres de archivo de las imágenes de entrada\n",
    "filenames = ['IMG_IZQ.JPG', 'IMG_MID.JPG', 'IMG_DER.JPG']\n",
    "H_filenames = ['H_IZQ.jpeg', 'H_MID.jpeg', 'H_DER.jpeg']\n",
    "V_filenames = ['V_IZQ.jpeg', 'V_MID.jpeg', 'V_DER.jpeg']\n",
    "profe_filenames = ['img1.png','img2.png','img3.png']\n",
    "\n",
    "# Read images (replace paths with your actual image paths)\n",
    "keypoints_left, descriptors_left, image_left = read_and_find_features(H_filenames[0])\n",
    "keypoints_mid, descriptors_mid, image_mid = read_and_find_features(H_filenames[1])\n",
    "keypoints_right, descriptors_right, image_right = read_and_find_features(H_filenames[2])\n",
    "\n",
    "# Create the panorama\n",
    "final_panorama = create_panorama(image_left, image_mid, image_right, keypoints_left, keypoints_mid, keypoints_right, descriptors_left, descriptors_mid, descriptors_right)\n",
    "\n",
    "# Display the final panorama\n",
    "cv2.imshow('Final Panorama', final_panorama)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the panorama image if needed\n",
    "cv2.imwrite('final_panorama.jpg', final_panorama)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIV_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
