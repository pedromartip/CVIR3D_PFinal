{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el detector ORB\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Inicializa el matcher\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "def compute_matches(detector, descriptor_matcher, img1, img2):\n",
    "    # Encuentra los puntos clave y los descriptores con ORB\n",
    "    kp1, des1 = detector.detectAndCompute(img1, None)\n",
    "    kp2, des2 = detector.detectAndCompute(img2, None)\n",
    "\n",
    "    # Empareja los descriptores\n",
    "    matches = descriptor_matcher.match(des1, des2)\n",
    "\n",
    "    # Filtra los emparejamientos con el test de ratio de Lowe\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    good_matches = matches[:int(len(matches) * 0.15)]  # Conserva solo el 15% de los mejores emparejamientos\n",
    "\n",
    "    return kp1, kp2, good_matches\n",
    "\n",
    "def stitch_images(img1, img2, kp1, kp2, matches):\n",
    "    # Extrae las ubicaciones de los puntos emparejados para la homografía\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Calcula la matriz de homografía\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Usa la homografía para transformar las esquinas de img1 a img2\n",
    "    h, w = img2.shape[:2]\n",
    "    pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "\n",
    "    # Encuentra los límites para la imagen final\n",
    "    [x_min, y_min] = np.int32(dst.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(dst.max(axis=0).ravel() + 0.5)\n",
    "    translation_dist = [-x_min, -y_min]\n",
    "\n",
    "    # Tamaño de la imagen de salida\n",
    "    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n",
    "\n",
    "    # Warp de la imagen de salida\n",
    "    output_img = cv2.warpPerspective(img1, H_translation.dot(H), (x_max - x_min, y_max - y_min))\n",
    "    output_img[translation_dist[1]:h + translation_dist[1], translation_dist[0]:w + translation_dist[0]] = img2\n",
    "\n",
    "    return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1600,1200) into shape (1600,650)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m kp_right, kp_central, matches_right_central \u001b[38;5;241m=\u001b[39m compute_matches(orb, bf, images[\u001b[38;5;241m2\u001b[39m], images[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Une la imagen izquierda a la central\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m left_to_central \u001b[38;5;241m=\u001b[39m \u001b[43mstitch_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_central\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches_left_central\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(left_to_central, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Une la imagen derecha a la central\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[59], line 44\u001b[0m, in \u001b[0;36mstitch_images\u001b[1;34m(img1, img2, kp1, kp2, matches)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Warp de la imagen de salida\u001b[39;00m\n\u001b[0;32m     43\u001b[0m output_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(img1, H_translation\u001b[38;5;241m.\u001b[39mdot(H), (x_max \u001b[38;5;241m-\u001b[39m x_min, y_max \u001b[38;5;241m-\u001b[39m y_min))\n\u001b[1;32m---> 44\u001b[0m output_img[translation_dist[\u001b[38;5;241m1\u001b[39m]:h \u001b[38;5;241m+\u001b[39m translation_dist[\u001b[38;5;241m1\u001b[39m], translation_dist[\u001b[38;5;241m0\u001b[39m]:w \u001b[38;5;241m+\u001b[39m translation_dist[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m img2\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_img\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (1600,1200) into shape (1600,650)"
     ]
    }
   ],
   "source": [
    "# Nombres de archivo de las imágenes de entrada\n",
    "filenames = ['IMG_IZQ.JPG', 'IMG_MID.JPG', 'IMG_DER.JPG']\n",
    "H_filenames = ['H_IZQ.jpeg', 'H_MID.jpeg', 'H_DER.jpeg']\n",
    "V_filenames = ['V_IZQ.jpeg', 'V_MID.jpeg', 'V_DER.jpeg']\n",
    "profe_filenames = ['img1.png','img2.png','img3.png']\n",
    "\n",
    "images = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in H_filenames]  # Asegúrate de cargarlas en escala de grises\n",
    "\n",
    "# Calcula los emparejamientos\n",
    "kp_left, kp_central, matches_left_central = compute_matches(orb, bf, images[0], images[1])\n",
    "kp_right, kp_central, matches_right_central = compute_matches(orb, bf, images[2], images[1])\n",
    "\n",
    "# Une la imagen izquierda a la central\n",
    "left_to_central = stitch_images(images[0], images[1], kp_left, kp_central, matches_left_central)\n",
    "plt.imshow(cv2.cvtColor(left_to_central, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# Une la imagen derecha a la central\n",
    "right_to_central = stitch_images(images[2], images[1], kp_right, kp_central, matches_right_central)\n",
    "plt.imshow(cv2.cvtColor(right_to_central, cv2.COLOR_BGR2RGB))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIV_P1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
